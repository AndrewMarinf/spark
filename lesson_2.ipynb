{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.master\", \"cluster\") \\ # Определяет режим запуска приложения (локальный, standalone, YARN, Mesos).\n",
    "    .config(\"spark.driver.memory\", '2g') \\ # Определяет объем памяти, выделенный для драйвера.\n",
    "    .config(\"spark.executor.instances\", 2) \\ # Определяет количество исполнителей (executor).\n",
    "    .config(\"spark.executor.cores\", '2') \\  # Определяет количество ядер, выделяемых для каждого исполнителя.\n",
    "    .config(\"spark.executor.memory\", '4g') \\ # Определяет объем памяти, выделенный для каждого исполнителя.\n",
    "    .appName(\"halltape_pyspark_local\") \\ # это будет написано в YARN\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "print(\"Активные Spark сессии:\", spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Создаем SparkSession с настройками\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"example_app\") \\  # Имя приложения\n",
    "    .master(\"yarn\") \\  # Режим запуска (YARN)\n",
    "    \n",
    "    # 1. Конфигурационные параметры Spark\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\  # Объем памяти для драйвера (2 гигабайта)\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\  # Объем памяти для каждого исполнителя (4 гигабайта)\n",
    "    .config(\"spark.executor.instances\", \"2\") \\  # Количество исполнителей (2)\n",
    "    .config(\"spark.default.parallelism\", \"200\") \\  # Количество партиций по умолчанию (200)\n",
    "    \n",
    "    # 2. Параметры управления ресурсами\n",
    "    .config(\"spark.cores.max\", \"4\") \\  # Максимальное количество ядер для приложения (4)\n",
    "    .config(\"spark.executor.cores\", \"2\") \\  # Количество ядер на исполнителя (2)\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\  # Включение динамического выделения ресурсов\n",
    "    \n",
    "    # 3. Параметры безопасности\n",
    "    .config(\"spark.authenticate\", \"true\") \\  # Включение аутентификации между компонентами\n",
    "    .config(\"spark.ssl.enabled\", \"true\") \\  # Включение SSL/TLS для защищенного соединения\n",
    "    \n",
    "    # 4. Параметры кэширования и хранения данных\n",
    "    .config(\"spark.local.dir\", \"/tmp/spark\") \\  # Путь для временных данных (shuffle, кэш)\n",
    "    .config(\"spark.storage.memoryFraction\", \"0.6\") \\  # Доля памяти для кэширования данных (60%)\n",
    "    \n",
    "    # 5. Параметры уровня журналирования и мониторинга\n",
    "    .config(\"spark.eventLog.enabled\", \"true\") \\  # Включение журналирования событий\n",
    "    .config(\"spark.executor.logs.rolling.maxRetainedFiles\", \"5\") \\  # Максимальное количество файлов журналов (5)\n",
    "    \n",
    "    .getOrCreate()  # Создание или получение существующей SparkSession\n",
    "\n",
    "# Проверка конфигурации\n",
    "print(\"Текущие настройки Spark:\")\n",
    "print(spark.sparkContext.getConf().getAll())\n",
    "\n",
    "# Пример работы с данными\n",
    "data = [(\"Alice\", 34), (\"Bob\", 45), (\"Cathy\", 29)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Вывод данных\n",
    "df.show()\n",
    "\n",
    "# Остановка SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''YARN это ресурс менеджер который разбивает и находит все ресурсы, говорит нам где и что у нас запущенно\n",
    "\n",
    "Driver нужен для того чтобы покозать инф с executor.. собирает инф с них . Он может находиться или отдельно на компе вне кластера или прям на сластаре\n",
    "драйвер управляет и собирает'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls  # какие есть папки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls dataset/ # это папка  открыть там три таблички"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd # покажет путь  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'ТУТ ПОЛНЫЙ ПУТЬ '\n",
    "\n",
    "card = spark.read.csv(f\"{PATH}/card.csv\",header=True, sep=';') # header что есть название колонок ,  sep= разделитель\n",
    "card = spark.read.csv(\"тут путь card.csv ИЛИ ПОЛНЫЙ ПУТЬ ЕСЛИ НЕ ПРОЧИТАЕТ\")\n",
    "date = '2020-06-01'\n",
    "card = spark.read.csv(f\"{PATH}/card.csv\",header=True, sep=';').where('''load_date=\"{date}\"''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'ТУТ ПОЛНЫЙ ПУТЬ '\n",
    "date = '2020-06-01'\n",
    "'''ТУТ ТРИ ТАБЛИЦЫ'''\n",
    "card = spark.read.csv(f\"{PATH}/card.csv\",header=True, sep=';').where('''load_date=\"{date}\"''') \n",
    "status = spark.read.csv(f\"{PATH}/status.csv\",header=True, sep=';').where('''load_date=\"{date}\"''')\n",
    "trx = spark.read.csv(f\"{PATH}/trx.csv\",header=True, sep=';').where('''load_date=\"{date}\"''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ВИТИРИНА ДАННЫХ\n",
    "ЗАДАЧА СЖОЙНИТЬ ЭТИ ТРИ ТАБЛИЦЫ И ВЫВЕСТИ НУЖЫНЕ ДАННЫЕ\n",
    "И ТУТ ПОЛЯ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trx_df = trx.groupBy('card_num').agg(F.min('amount').alias('amount')).where('''card_num = \"345678\"''')\n",
    "trx_df.show()\n",
    "# мы тут сгруппировали стобец card_num где, столбец amount самый min.. \n",
    "# и вывели только конкрентную карту 345678 в стобце card_num "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trx_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trx_df_p = trx_df.join(trx, ['card_num', 'transaction_datetime'], 'inner').columns\n",
    "# если мы просто хотим посмотреть какие колонки вызовутся при джоине, то саму оперцию не вызовет и ресурсы жрать не будет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# джойним наш датафрейм trx_df с второй таблицей status\n",
    "df_2 = status.join(trx_df, 'card_num', 'inner').show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# джойним третию таблицу \n",
    "df_2.join(card, 'card_num_md5', 'inner').show() # ТУТ ОНА БУДЕТ СЛИШКОМ БОЛЬШАЯ И ПО ЭТОМУ МЫ ПИШМ ТАК \n",
    "result = df_2(()).join(card, 'card_num_md5', 'inner').show(5,False,True) # он перевернет в столбики слево и будет удобно смотреть"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
