{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c511af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d3fc52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/18 08:06:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/18 08:06:05 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Активные Spark сессии: http://87fa079ba304:4041\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"halltape_pyspark_local\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "print(\"Активные Spark сессии:\", spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27bda28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Активные Spark сессии: http://87fa079ba304:4041\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Настройка окружения для PySpark\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Создание SparkSession\n",
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"halltape_pyspark_local\") \\\n",
    "            .master(\"local[*]\")  \\\n",
    "            .config(\"spark.ui.port\", \"4040\")   \\\n",
    "            .getOrCreate()\n",
    "\n",
    "print(\"Активные Spark сессии:\", spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "530c86bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/customs_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6860d171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/customs_data.csv\n"
     ]
    }
   ],
   "source": [
    "print(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ba58fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                 _c0|\n",
      "+--------------------+\n",
      "|month;country;cod...|\n",
      "|01/2016;IT;620469...|\n",
      "|01/2016;CN;900190...|\n",
      "|01/2016;BY;841430...|\n",
      "|01/2016;US;901850...|\n",
      "|01/2016;EE;902110...|\n",
      "|01/2016;FR;381600...|\n",
      "|01/2016;MX;852351...|\n",
      "|01/2016;JP;620452...|\n",
      "|01/2016;KR;611020...|\n",
      "|01/2016;KG;852713...|\n",
      "|01/2016;ZA;842123...|\n",
      "|01/2016;CN;851810...|\n",
      "|01/2016;TR;841790...|\n",
      "|01/2016;IT;390610...|\n",
      "|01/2016;CZ;870840...|\n",
      "|01/2016;ES;640419...|\n",
      "|01/2016;IT;940490...|\n",
      "|01/2016;UA;820780...|\n",
      "|01/2016;CN;330410...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(PATH).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e138b571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+------+-----+--------+------+--------+-------------+-----------+--------------------+\n",
      "|  month|country|      code| value|netto|quantity|region|district|direction_eng|measure_eng|           load_date|\n",
      "+-------+-------+----------+------+-----+--------+------+--------+-------------+-----------+--------------------+\n",
      "|01/2016|     IT|6204695000|   131|    1|       7| 46000|      01|           IM|        ShT|2024-07-01T00:00:...|\n",
      "|01/2016|     CN|9001900009|112750|   18|       0| 46000|      01|           IM|          1|2024-01-01T00:00:...|\n",
      "|01/2016|     BY|8414302004|   392|   57|       8| 50000|      06|           IM|        ShT|2024-06-01T00:00:...|\n",
      "|01/2016|     US|9018509000| 54349|  179|       0| 40000|      02|           IM|          1|2024-04-01T00:00:...|\n",
      "|01/2016|     EE|9021101000| 17304|  372|       0| 46000|      01|           IM|          1|2024-02-01T00:00:...|\n",
      "+-------+-------+----------+------+-----+--------+------+--------+-------------+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(PATH, sep=';', header=True).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76134e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------\n",
      " month         | 01/2016                       \n",
      " country       | IT                            \n",
      " code          | 6204695000                    \n",
      " value         | 131                           \n",
      " netto         | 1                             \n",
      " quantity      | 7                             \n",
      " region        | 46000                         \n",
      " district      | 01                            \n",
      " direction_eng | IM                            \n",
      " measure_eng   | ShT                           \n",
      " load_date     | 2024-07-01T00:00:00.000+03:00 \n",
      "-RECORD 1--------------------------------------\n",
      " month         | 01/2016                       \n",
      " country       | CN                            \n",
      " code          | 9001900009                    \n",
      " value         | 112750                        \n",
      " netto         | 18                            \n",
      " quantity      | 0                             \n",
      " region        | 46000                         \n",
      " district      | 01                            \n",
      " direction_eng | IM                            \n",
      " measure_eng   | 1                             \n",
      " load_date     | 2024-01-01T00:00:00.000+03:00 \n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(PATH, sep=';', header=True)\n",
    "\n",
    "# df.show(trancat=False) # чтобы влезло все, если длинное название столбца\n",
    "\n",
    "df.show(2, False, True) # перевернем столбецы ВЕРТИКАЛЬНО и выведем только 2 строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0196420b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- code: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- netto: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- direction_eng: string (nullable = true)\n",
      " |-- measure_eng: string (nullable = true)\n",
      " |-- load_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema() # Это схема данных вашего DataFrame.  структура и типы данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40579ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['month',\n",
       " 'country',\n",
       " 'code',\n",
       " 'value',\n",
       " 'netto',\n",
       " 'quantity',\n",
       " 'region',\n",
       " 'district',\n",
       " 'direction',\n",
       " 'measure',\n",
       " 'load_date']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = df.withColumnRenamed(\"direction_eng\", \"direction\").withColumnRenamed(\"measure_eng\",\"measure\")\n",
    "# result.printSchema() # структура и типы данных\n",
    "result.columns  # вытащить какие колонки есть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "588677a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=====================>                                   (6 + 10) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|country|\n",
      "+-------+\n",
      "|LT     |\n",
      "|MM     |\n",
      "|DZ     |\n",
      "|CI     |\n",
      "|TC     |\n",
      "|FI     |\n",
      "|SC     |\n",
      "|AZ     |\n",
      "|PM     |\n",
      "|UA     |\n",
      "+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result\\\n",
    "        .select('country')\\\n",
    "        .distinct()\\\n",
    "        .show(10,truncate=False )\n",
    "\n",
    "# result.select('country').distinct().show(10, truncate=False) ИЛИ ТАК ПИСАТЬ\n",
    "\n",
    "# 16 ЭТО ПАРТИЦИЙ, SPARK РАЗДРАБИЛ НА 16 И НА ОДНОМ КОМПЕ СЧИТАЕТ, А ЕСЛИ БЫ БЫ КЛАСТЕР ОН БЫ КАЖДУЮ ПАРТИЦИЮ СЧИТАЛ НА РАЗНОМ КОМПЕ ПАРАЛЛЕЛЬНО"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "449c3f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+------+------+--------+------+--------+---------+-------+--------------------+\n",
      "|  month|country|      code| value| netto|quantity|region|district|direction|measure|           load_date|\n",
      "+-------+-------+----------+------+------+--------+------+--------+---------+-------+--------------------+\n",
      "|01/2016|     IT|6204695000|   131|     1|       7| 46000|      01|       IM|    ShT|2024-07-01T00:00:...|\n",
      "|01/2016|     CN|9001900009|112750|    18|       0| 46000|      01|       IM|      1|2024-01-01T00:00:...|\n",
      "|01/2016|     BY|8414302004|   392|    57|       8| 50000|      06|       IM|    ShT|2024-06-01T00:00:...|\n",
      "|01/2016|     US|9018509000| 54349|   179|       0| 40000|      02|       IM|      1|2024-04-01T00:00:...|\n",
      "|01/2016|     EE|9021101000| 17304|   372|       0| 46000|      01|       IM|      1|2024-02-01T00:00:...|\n",
      "|01/2016|     FR|3816000000|323488|253600|       0| 40000|      02|       IM|      1|2024-02-01T00:00:...|\n",
      "|01/2016|     MX|8523519300|  1611|     0|       4| 40000|      02|       IM|    ShT|2024-04-01T00:00:...|\n",
      "|01/2016|     JP|6204520000|    29|     1|       2| 46000|      01|       IM|    ShT|2024-02-01T00:00:...|\n",
      "|01/2016|     KR|6110209100|   815|     2|       5| 46000|      01|       IM|    ShT|2024-01-01T00:00:...|\n",
      "|01/2016|     KG|8527139900| 11868|  2127|    2630| 46000|      01|       IM|    ShT|2024-06-01T00:00:...|\n",
      "|01/2016|     ZA|8421230000| 12686|  1785|    3451| 45000|      01|       IM|    ShT|2024-03-01T00:00:...|\n",
      "|01/2016|     CN|8518109500|    12|     0|      10| 65000|      05|       IM|    ShT|2024-03-01T00:00:...|\n",
      "|01/2016|     TR|8417900000|206453| 17297|       1| 92000|      04|       IM|    ShT|2024-06-01T00:00:...|\n",
      "|01/2016|     IT|3906100000|  4492|  1075|       0| 45000|      01|       IM|      1|2024-02-01T00:00:...|\n",
      "|01/2016|     CZ|8708409909|    41|     2|       0| 46000|      01|       IM|      1|2024-01-01T00:00:...|\n",
      "|01/2016|     ES|6404191000| 11822|   346|     760| 45000|      01|       IM|    PAR|2024-01-01T00:00:...|\n",
      "|01/2016|     IT|9404909000|  6801|   485|       0| 46000|      01|       IM|      1|2024-03-01T00:00:...|\n",
      "|01/2016|     UA|8207801900| 35793|  1020|       0| 14000|      01|       IM|      1|2024-02-01T00:00:...|\n",
      "|01/2016|     CN|3304100000| 59678| 10829|       0| 46000|      01|       IM|      1|2024-06-01T00:00:...|\n",
      "|01/2016|     SI|6104440000|  1470|    13|      15| 45000|      01|       IM|    ShT|2024-05-01T00:00:...|\n",
      "+-------+-------+----------+------+------+--------+------+--------+---------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb3f52e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:============================>                            (8 + 8) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|country|total_rows|\n",
      "+-------+----------+\n",
      "|     BY|   3509568|\n",
      "|     KZ|   2519896|\n",
      "|     CN|   2454792|\n",
      "|     DE|   1542311|\n",
      "|     UA|   1158498|\n",
      "|     IT|   1102837|\n",
      "|     US|    835936|\n",
      "|     PL|    666690|\n",
      "|     FR|    593040|\n",
      "|     JP|    571756|\n",
      "|     TR|    463432|\n",
      "|     KR|    446907|\n",
      "|     GB|    443091|\n",
      "|     AM|    438705|\n",
      "|     CZ|    407360|\n",
      "|     KG|    403565|\n",
      "|     ES|    401644|\n",
      "|     IN|    374151|\n",
      "|     NL|    365193|\n",
      "|     UZ|    329707|\n",
      "+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result\\\n",
    "        .groupBy('country')\\\n",
    "        .agg(F.count('*').alias('total_rows'))\\\n",
    "        .orderBy(F.col('total_rows').desc())\\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66bc40a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+------+-----+--------+------+--------+---------+-------+--------------------+\n",
      "|  month|country|      code| value|netto|quantity|region|district|direction|measure|           load_date|\n",
      "+-------+-------+----------+------+-----+--------+------+--------+---------+-------+--------------------+\n",
      "|01/2016|     IT|6204695000|   131|    1|       7| 46000|      01|       IM|    ShT|2024-07-01T00:00:...|\n",
      "|01/2016|     CN|9001900009|112750|   18|       0| 46000|      01|       IM|      1|2024-01-01T00:00:...|\n",
      "+-------+-------+----------+------+-----+--------+------+--------+---------+-------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5300e223",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                       (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 2 ВАРИАНТА ФИЛЬТРОВАТЬ\n",
    "\n",
    "df_1 = result\\\n",
    "                .where(F.col('country') == 'DE' )\\\n",
    "                .where(F.col('value').isNotNull())\n",
    "# df_1.show()\n",
    "\n",
    "df_2 = result\\\n",
    "                .where('''country == \"DE\"''')\\\n",
    "                .where('''value IS NOT NULL ''')\n",
    "# df_2.show()\n",
    "\n",
    "print(df_1.count() == df_2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648fbd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+------+-----+--------+------+--------+---------+-------+----------+\n",
      "|month  |country|code      |value |netto|quantity|region|district|direction|measure|load_date |\n",
      "+-------+-------+----------+------+-----+--------+------+--------+---------+-------+----------+\n",
      "|01/2016|DE     |4016995709|5901  |172  |0       |46000 |01      |IM       |1      |2024-01-01|\n",
      "|01/2016|DE     |8708809109|1213  |94   |0       |45000 |01      |IM       |1      |2024-01-01|\n",
      "|01/2016|DE     |7013419000|7020  |1611 |492     |45000 |01      |IM       |ShT    |2024-02-01|\n",
      "|01/2016|DE     |3923309090|46294 |8048 |0       |45000 |01      |IM       |1      |2024-04-01|\n",
      "|01/2016|DE     |4015190000|440008|73357|6463937 |45000 |01      |IM       |PAR    |2024-03-01|\n",
      "+-------+-------+----------+------+-----+--------+------+--------+---------+-------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final = df_2\\\n",
    "            .select('month',\n",
    "                    'country',\n",
    "                   'code',\n",
    "                   'value',\n",
    "                   'netto',\n",
    "                   'quantity',\n",
    "                   'region',\n",
    "                   'district',\n",
    "                   'direction',\n",
    "                   'measure',\n",
    "                   F.col('load_date').cast('date')) # убрали время и оставили время\n",
    "\n",
    "final.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afc28888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- code: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      " |-- netto: string (nullable = true)\n",
      " |-- quantity: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- district: string (nullable = true)\n",
      " |-- direction: string (nullable = true)\n",
      " |-- measure: string (nullable = true)\n",
      " |-- load_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7f2a996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+------+-----+--------+------+--------+---------+-------+----------+\n",
      "|  month|country|      code| value|netto|quantity|region|district|direction|measure| load_date|\n",
      "+-------+-------+----------+------+-----+--------+------+--------+---------+-------+----------+\n",
      "|01/2016|     DE|4016995709|  5901|  172|       0| 46000|      01|       IM|      1|2024-01-01|\n",
      "|01/2016|     DE|8708809109|  1213|   94|       0| 45000|      01|       IM|      1|2024-01-01|\n",
      "|01/2016|     DE|7013419000|  7020| 1611|     492| 45000|      01|       IM|    ShT|2024-02-01|\n",
      "|01/2016|     DE|3923309090| 46294| 8048|       0| 45000|      01|       IM|      1|2024-04-01|\n",
      "|01/2016|     DE|4015190000|440008|73357| 6463937| 45000|      01|       IM|    PAR|2024-03-01|\n",
      "|01/2016|     DE|2918140000|   265|   15|       0| 45000|      01|       IM|      1|2024-03-01|\n",
      "|01/2016|     DE|3208101000| 10075|  680|       0| 78000|      01|       IM|      1|2024-06-01|\n",
      "|01/2016|     DE|7608100009|  5515|   12|       0| 92000|      04|       IM|      1|2024-03-01|\n",
      "|01/2016|     DE|8505909000|  1141|   11|       0| 46000|      01|       IM|      1|2024-02-01|\n",
      "|01/2016|     DE|5602101900| 18483| 3520|       0| 42000|      01|       IM|      1|2024-02-01|\n",
      "|01/2016|     DE|3405909000| 48947|19440|       0| 07000|      08|       IM|      1|2024-02-01|\n",
      "|01/2016|     DE|7306408008|   403|   16|       0| 70000|      01|       IM|      1|2024-01-01|\n",
      "|01/2016|     DE|2106909803|   497|   66|       0| 40000|      02|       IM|      1|2024-04-01|\n",
      "|01/2016|     DE|2918199800| 14231| 2000|       0| 40000|      02|       IM|      1|2024-02-01|\n",
      "|01/2016|     DE|8536508000|  7920|   62|       0| 65000|      05|       IM|      1|2024-03-01|\n",
      "|01/2016|     DE|8481309909|  5652|   58|       0| 46000|      01|       IM|      1|2024-04-01|\n",
      "|01/2016|     DE|3506910000| 10466| 1800|       0| 60000|      03|       IM|      1|2024-01-01|\n",
      "|01/2016|     DE|5906999000|   846|   60|       0| 45000|      01|       IM|      1|2024-01-01|\n",
      "|01/2016|     DE|8207709000| 16308|  148|       0| 27000|      02|       IM|      1|2024-01-01|\n",
      "|01/2016|     DE|3824400000| 41312| 4670|       0| 46000|      01|       IM|      1|2024-03-01|\n",
      "+-------+-------+----------+------+-----+--------+------+--------+---------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d6cbe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во партиции 16\n"
     ]
    }
   ],
   "source": [
    "# КАК СОХРАНЯТЬ ??  некотроллируемое по колличеству файлов \n",
    "\n",
    "# 1 вариант  сохрание некотроллируемое по колличеству файлов  и в папке будет несколько (файлов партиции) \n",
    "final\\\n",
    "    .write\\\n",
    "    .format('csv')\\\n",
    "    .options(header='True', sep=';')\\\n",
    "    .csv('data/final_no_control') \n",
    "    \n",
    "partition_num = final.rdd.getNumPartitions() # это узнать на какое колличество партиций мы записали\n",
    "print(f'Кол-во партиции {partition_num}')               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a507433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во партиции 16\n"
     ]
    }
   ],
   "source": [
    "# КАК СОХРАНЯТЬ ??  как один или несколько файлов\n",
    "\n",
    "# сохранение контроллируемое по кол-ву файлов - ОДИН ФАЙЛ мы указали в coalesce(1)\n",
    "# есть еще вариант написать repartition(1) схлопни в одмин\n",
    "# ЕСЛИ МЫ ПИШЕМ РАЗНЫЕ coalesce(4) = ПАРТИЦИИ БУДУТ РАЗНОГО РАЗМЕРА И repartition(1) = ПАРТИЦИИ БУДУТ ПРИМЕРНО ВСЕ ОДИНАКОВЫ ПО РАЗМЕРУ, НО ЭТО ОПЕРЦИИ ТЯЖЕЛЕЕ ТАК КАК БУДУТ ДОП \n",
    "final\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .format('csv')\\\n",
    "    .options(header='True', sep=';')\\\n",
    "    .csv('data/final_one_file') \n",
    "    \n",
    "partition_df = final.rdd.getNumPartitions() # это узнать на какое колличество партиций мы записали\n",
    "print(f'Кол-во партиции {partition_df}')              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1acef80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "| load_date|\n",
      "+----------+\n",
      "|2024-08-01|\n",
      "|2024-09-01|\n",
      "|2024-10-01|\n",
      "|2024-04-01|\n",
      "|2024-05-01|\n",
      "|2024-07-01|\n",
      "|2024-02-01|\n",
      "|2024-03-01|\n",
      "|2024-01-01|\n",
      "|2024-06-01|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final.select('load_date').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8dce7aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_date distinct:10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# КАК СОХРАНЯТЬ ?? с партицированием, то есть по условию какому нибудь почти как wherе..там \n",
    "\n",
    "final\\\n",
    "    .write\\\n",
    "    .partitionBy('load_date')\\\n",
    "    .format('csv')\\\n",
    "    .options(header='True', sep=';')\\\n",
    "    .csv('data/final_partitioned')\n",
    "\n",
    "print_df = final.select('load_date').distinct()\n",
    "print(f'load_date distinct:{print_df.count()}')            \n",
    "        \n",
    "        \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "43b69438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во партиций 1\n"
     ]
    }
   ],
   "source": [
    " # КАК СОХРАНЯТЬ ?? чтобы было не несколько файлов в папке а один файл и только с одним криетрием load_date и столбца load_date там не будет, он какбы в общем по всем папкаи и название папки это и есть строка  \n",
    " # # Сохранения с партицированием и repartition внутри самой партиции\n",
    "final\\\n",
    "    .repartition(1, 'load_date')\\\n",
    "    .write\\\n",
    "    .partitionBy('load_date')\\\n",
    "    .format('csv')\\\n",
    "    .options(header='True', sep=';')\\\n",
    "    .csv('data/final_partitioned_repart')\n",
    "\n",
    "partition_num = final.repartition(1, 'load_date').rdd.getNumPartitions()\n",
    "print(f'Кол-во партиций {partition_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90dab04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350998"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# как читать???  тут он прочитал 16 мелких файлов.. это то что мы сохранили\n",
    "reader_no_control = spark\\\n",
    "                        .read\\\n",
    "                        .csv('data/final_no_control/', header=True, sep=';')\\\n",
    "                        .where(''' load_date = \"2024-01-01\" ''')\n",
    "reader_no_control.count()                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0b2601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350998"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тут мы читает один файл который мы сохранили\n",
    "reader_final_one_file = spark\\\n",
    "                            .read\\\n",
    "                            .csv('data/final_one_file/', header=True, sep=';')\\\n",
    "                            .where(''' load_date = \"2024-01-01\" ''')\n",
    "reader_final_one_file.count() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90e55bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350998"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# тут где партиция и мног файлов\n",
    "reader_partitioned = spark\\\n",
    "                        .read\\\n",
    "                        .csv('data/final_partitioned', header=True, sep=';')\\\n",
    "                        .where(''' load_date = \"2024-01-01\" ''')\n",
    "                        \n",
    "reader_partitioned.count()                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a03508e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350998"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader_partitioned_repart = spark\\\n",
    "                                .read\\\n",
    "                                .csv('data/final_partitioned_repart', header=True, sep=';')\\\n",
    "                                .where(''' load_date = \"2024-01-01\" ''')\n",
    "reader_partitioned_repart.count()                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b7d76c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|region_id|       name|\n",
      "+---------+-----------+\n",
      "|    14000|   Северный|\n",
      "|    11000|      Южный|\n",
      "|    10000|  Восточный|\n",
      "|    26000|   Западный|\n",
      "|    56000|Центральный|\n",
      "+---------+-----------+\n",
      "\n",
      "+-------+-------+----------+------+-----+--------+------+--------+-------------+-----------+--------------------+\n",
      "|  month|country|      code| value|netto|quantity|region|district|direction_eng|measure_eng|           load_date|\n",
      "+-------+-------+----------+------+-----+--------+------+--------+-------------+-----------+--------------------+\n",
      "|01/2016|     IT|6204695000|   131|    1|       7| 46000|      01|           IM|        ShT|2024-07-01T00:00:...|\n",
      "|01/2016|     CN|9001900009|112750|   18|       0| 46000|      01|           IM|          1|2024-01-01T00:00:...|\n",
      "+-------+-------+----------+------+-----+--------+------+--------+-------------+-----------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (14000, \"Северный\"),\n",
    "    (11000, \"Южный\"),\n",
    "    (10000, \"Восточный\"),\n",
    "    (26000, \"Западный\"),\n",
    "    (56000, \"Центральный\")\n",
    "]\n",
    "\n",
    "region_df = spark.createDataFrame(data, schema='region_id long, name string')\n",
    "\n",
    "region_df.show()\n",
    "\n",
    "\n",
    "customs_data = spark\\\n",
    "                .read\\\n",
    "                .csv('data/customs_data.csv', header=True, sep=';')\n",
    "\n",
    "customs_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6315b426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отключим автоматический Broadcast JOIN  тут мы отключаем Broadcast..джойним как обычно без копии нашего DF к большой табличке\n",
    "import time\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fff43c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for join operation: 18.63 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Замерим выполнение запроса без broadcast join\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "customs_data.join(region_df, customs_data.region==region_df.region_id, \"left\").count()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Elapsed time for join operation: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfac4b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 42:===>                                                    (1 + 15) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time for broadcast join operation: 11.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Замерим выполнение запроса c broadcast join.. тут мы копию нашего DF отправляем на каждый excuter и он там join с своим мелким кусочком\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "customs_data.join(F.broadcast(region_df), customs_data.region == region_df.region_id, \"left\").count()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Elapsed time for broadcast join operation: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2154503",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/18 11:15:51 WARN MemoryStore: Not enough space to cache rdd_177_4 in memory! (computed 20.7 MiB so far)\n",
      "25/02/18 11:15:51 WARN BlockManager: Persisting block rdd_177_4 to disk instead.\n",
      "25/02/18 11:15:51 WARN MemoryStore: Not enough space to cache rdd_177_13 in memory! (computed 18.9 MiB so far)\n",
      "25/02/18 11:15:51 WARN BlockManager: Persisting block rdd_177_13 to disk instead.\n",
      "25/02/18 11:15:51 WARN MemoryStore: Not enough space to cache rdd_177_3 in memory! (computed 19.5 MiB so far)\n",
      "25/02/18 11:15:51 WARN BlockManager: Persisting block rdd_177_3 to disk instead.\n",
      "25/02/18 11:15:51 WARN MemoryStore: Not enough space to cache rdd_177_1 in memory! (computed 19.8 MiB so far)\n",
      "25/02/18 11:15:51 WARN BlockManager: Persisting block rdd_177_1 to disk instead.\n",
      "25/02/18 11:15:56 WARN MemoryStore: Not enough space to cache rdd_177_2 in memory! (computed 25.0 MiB so far)\n",
      "25/02/18 11:15:56 WARN BlockManager: Persisting block rdd_177_2 to disk instead.\n",
      "25/02/18 11:16:01 WARN MemoryStore: Not enough space to cache rdd_177_8 in memory! (computed 28.6 MiB so far)\n",
      "25/02/18 11:16:01 WARN BlockManager: Persisting block rdd_177_8 to disk instead.\n",
      "25/02/18 11:16:01 WARN MemoryStore: Not enough space to cache rdd_177_10 in memory! (computed 29.4 MiB so far)\n",
      "25/02/18 11:16:01 WARN BlockManager: Persisting block rdd_177_10 to disk instead.\n",
      "25/02/18 11:16:07 WARN MemoryStore: Not enough space to cache rdd_177_7 in memory! (computed 34.1 MiB so far)\n",
      "25/02/18 11:16:07 WARN BlockManager: Persisting block rdd_177_7 to disk instead.\n",
      "25/02/18 11:16:07 WARN MemoryStore: Not enough space to cache rdd_177_9 in memory! (computed 34.5 MiB so far)\n",
      "25/02/18 11:16:07 WARN BlockManager: Persisting block rdd_177_9 to disk instead.\n",
      "25/02/18 11:16:16 WARN MemoryStore: Not enough space to cache rdd_177_15 in memory! (computed 43.7 MiB so far)\n",
      "25/02/18 11:16:16 WARN BlockManager: Persisting block rdd_177_15 to disk instead.\n",
      "25/02/18 11:16:21 WARN MemoryStore: Not enough space to cache rdd_177_15 in memory! (computed 19.3 MiB so far)\n",
      "25/02/18 11:16:23 WARN MemoryStore: Not enough space to cache rdd_177_10 in memory! (computed 29.4 MiB so far)\n",
      "25/02/18 11:16:23 WARN MemoryStore: Not enough space to cache rdd_177_8 in memory! (computed 19.0 MiB so far)\n",
      "25/02/18 11:16:24 WARN MemoryStore: Not enough space to cache rdd_177_7 in memory! (computed 34.1 MiB so far)\n",
      "25/02/18 11:16:24 WARN MemoryStore: Not enough space to cache rdd_177_9 in memory! (computed 34.5 MiB so far)\n",
      "25/02/18 11:16:24 WARN MemoryStore: Not enough space to cache rdd_177_13 in memory! (computed 32.9 MiB so far)\n",
      "25/02/18 11:16:25 WARN MemoryStore: Not enough space to cache rdd_177_3 in memory! (computed 29.7 MiB so far)\n",
      "25/02/18 11:16:25 WARN MemoryStore: Not enough space to cache rdd_177_2 in memory! (computed 9.8 MiB so far)\n",
      "25/02/18 11:16:25 WARN MemoryStore: Not enough space to cache rdd_177_1 in memory! (computed 10.5 MiB so far)\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "26392290"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customs_data.cache().count()   # заносим в кеш"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79568e67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[month: string, country: string, code: string, value: string, netto: string, quantity: string, region: string, district: string, direction_eng: string, measure_eng: string, load_date: string]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customs_data.unpersist() # УДАЛЯЕМ ИЗ КЕША"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a598d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "customs_data.persist(StorageLevel.DISK_ONLY).count() # ТУТ ГОВОРИМ СОХРАНИ НА ДИСК ИЛИ КУДА ЕШЕ  \n",
    "\n",
    "# ЕСТЬ cache() В ОПЕРАТИВКУ СОХРАНЯЕТ,  ЕСТЬ persist() НА ДИСК СОХРАНИ И В ПАМЯТЬ ИЛИ ТОЛЬКО НА ДИСК "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f593a22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "| id|number|\n",
      "+---+------+\n",
      "|  1|   one|\n",
      "|  2|   two|\n",
      "|  3| three|\n",
      "|  4|  four|\n",
      "|  5|  five|\n",
      "|  6|   six|\n",
      "|  7| seven|\n",
      "|  8| eight|\n",
      "|  9|  nine|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(1,'one'), (2,'two'), (3,'three'), (4,'four'),\n",
    "        (5,'five'), (6,'six'), (7, 'seven'), (8, 'eight'),\n",
    "        (9, 'nine')]\n",
    "\n",
    "df = spark.createDataFrame(data, ['id', 'number'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "767c64d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [Row(id=1, number='one'),\n",
       "  Row(id=2, number='two'),\n",
       "  Row(id=3, number='three'),\n",
       "  Row(id=4, number='four'),\n",
       "  Row(id=5, number='five'),\n",
       "  Row(id=6, number='six'),\n",
       "  Row(id=7, number='seven'),\n",
       "  Row(id=8, number='eight'),\n",
       "  Row(id=9, number='nine')],\n",
       " []]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Намеренно перемешаем и поделим на 8 разделов\n",
    "mix = df.repartition(8)\n",
    "mix.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a812ce33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Row(id=2, number='two'),\n",
       "  Row(id=3, number='three'),\n",
       "  Row(id=8, number='eight')],\n",
       " [Row(id=1, number='one'),\n",
       "  Row(id=6, number='six'),\n",
       "  Row(id=7, number='seven'),\n",
       "  Row(id=9, number='nine')],\n",
       " [Row(id=4, number='four'), Row(id=5, number='five')]]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix.repartition(3).rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f71e7498",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [],\n",
       " [Row(id=1, number='one'),\n",
       "  Row(id=2, number='two'),\n",
       "  Row(id=3, number='three'),\n",
       "  Row(id=4, number='four'),\n",
       "  Row(id=5, number='five'),\n",
       "  Row(id=6, number='six'),\n",
       "  Row(id=7, number='seven'),\n",
       "  Row(id=8, number='eight'),\n",
       "  Row(id=9, number='nine')]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mix.coalesce(3).rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7c830537",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Pandas >= 0.23.2 must be installed; however, it was not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/conversion.py:63\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, DataFrame)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[0;32m---> 63\u001b[0m \u001b[43mrequire_minimum_pandas_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/sql/pandas/utils.py:32\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m     have_pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m have_pandas:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas >= \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must be installed; however, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     33\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit was not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m minimum_pandas_version)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LooseVersion(pandas\u001b[38;5;241m.\u001b[39m__version__) \u001b[38;5;241m<\u001b[39m LooseVersion(minimum_pandas_version):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPandas >= \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m must be installed; however, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour version was \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (minimum_pandas_version, pandas\u001b[38;5;241m.\u001b[39m__version__))\n",
      "\u001b[0;31mImportError\u001b[0m: Pandas >= 0.23.2 must be installed; however, it was not found."
     ]
    }
   ],
   "source": [
    "mix.toPandas().head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
