{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d81ed53d",
   "metadata": {},
   "source": [
    "### Start SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f518567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"halltape_pyspark_local\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "print(\"Активные Spark сессии:\", spark.sparkContext.uiWebUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f694b057",
   "metadata": {},
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c261f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data/customs_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f83366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(PATH).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "96401ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.csv(PATH, sep=';', header=True).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "19576866",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(PATH, sep=';', header=True)\n",
    "\n",
    "df.show(truncate=False)\n",
    "\n",
    "df.show(2, False, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23b149e",
   "metadata": {},
   "source": [
    "### PrintSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5c060c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "28f0874a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df\\\n",
    "            .withColumnRenamed(\"direction_eng\", \"direction\")\\\n",
    "            .withColumnRenamed(\"measure_eng\", \"measure\")\n",
    "\n",
    "result.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0b85ee",
   "metadata": {},
   "source": [
    "### Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "47004eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.select('country').distinct().show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30892869",
   "metadata": {},
   "source": [
    "### GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4738ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result\\\n",
    "    .groupBy('country')\\\n",
    "    .agg(F.count('*').alias('total_rows'))\\\n",
    "    .orderBy(F.col('total_rows').desc())\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24170772",
   "metadata": {},
   "source": [
    "### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "43c234ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Два варианта написании фильтрации\n",
    "df_de = result\\\n",
    "            .where(F.col('country') == 'DE')\\\n",
    "            .where(F.col('value').isNotNull())\n",
    "\n",
    "df_de2 = result\\\n",
    "            .where(''' country == \"DE\" ''')\\\n",
    "            .where(''' value IS NOT NULL ''')\n",
    "\n",
    "print(df_de.count() == df_de2.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420d9f4",
   "metadata": {},
   "source": [
    "### Save to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "81216a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_de.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "85691fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_de.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52281ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final = df_de\\\n",
    "            .select('month',\n",
    "                    'country',\n",
    "                    'code',\n",
    "                    'value',\n",
    "                    'netto',\n",
    "                    'quantity',\n",
    "                    'region',\n",
    "                    'district',\n",
    "                    'direction',\n",
    "                    'measure',\n",
    "                    F.col('load_date').cast('date'))\n",
    "\n",
    "final.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4a776d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение неконтроллируемое по кол-ву файлов\n",
    "final\\\n",
    "    .write\\\n",
    "    .format('csv')\\\n",
    "    .options(header='True', sep=';')\\\n",
    "    .csv('data/final_no_control')\n",
    "\n",
    "partition_num = final.rdd.getNumPartitions()\n",
    "print(f'Кол-во партиций {partition_num}')\n",
    "\n",
    "# Сохранение контроллируемое по кол-ву файлов - ОДИН ФАЙЛ\n",
    "final\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .format('csv')\\\n",
    "    .options(header='True', sep=';')\\\n",
    "    .csv('data/final_one_file') \n",
    "\n",
    "partition_num = final.coalesce(1).rdd.getNumPartitions()\n",
    "print(f'Кол-во партиций {partition_num}')\n",
    "\n",
    "\n",
    "# Сохранения с партицированием\n",
    "final\\\n",
    "    .write\\\n",
    "    .partitionBy('load_date')\\\n",
    "    .format('csv')\\\n",
    "    .options(header='True', sep=';')\\\n",
    "    .csv('data/final_partitioned')\n",
    "\n",
    "print_df = final.select('load_date').distinct()\n",
    "print(f'Load_date distinct: {print_df.count()}')\n",
    "\n",
    "\n",
    "# Сохранения с партицированием и repartition внутри самой партиции\n",
    "final\\\n",
    "    .repartition(1, 'load_date')\\\n",
    "    .write\\\n",
    "    .partitionBy('load_date')\\\n",
    "    .format('csv')\\\n",
    "    .options(header='True', sep=';')\\\n",
    "    .csv('data/final_partitioned_repart')\n",
    "\n",
    "partition_num = final.repartition(1, 'load_date').rdd.getNumPartitions()\n",
    "print(f'Кол-во партиций {partition_num}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad177d17",
   "metadata": {},
   "source": [
    "### Read Transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f8415e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_no_control = spark\\\n",
    "                        .read\\\n",
    "                        .csv('data/final_no_control/', header=True, sep=';')\\\n",
    "                        .where(''' load_date = \"2024-01-01\" ''')\n",
    "\n",
    "reader_final_one_file = spark\\\n",
    "                            .read\\\n",
    "                            .csv('data/final_one_file/', header=True, sep=';')\\\n",
    "                            .where(''' load_date = \"2024-01-01\" ''')\n",
    "\n",
    "reader_partitioned = spark\\\n",
    "                        .read\\\n",
    "                        .csv('data/final_partitioned', header=True, sep=';')\\\n",
    "                        .where(''' load_date = \"2024-01-01\" ''')\n",
    "\n",
    "reader_partitioned_repart = spark\\\n",
    "                                .read\\\n",
    "                                .csv('data/final_partitioned_repart', header=True, sep=';')\\\n",
    "                                .where(''' load_date = \"2024-01-01\" ''')\n",
    "\n",
    "\n",
    "reader_no_control.count() # number of files read: 16 | size of files read: 88.4 MiB | 2.5 s (90 ms, 301 ms, 384 ms)\n",
    "\n",
    "reader_final_one_file.count() # number of files read: 1 | size of files read: 88.4 MiB | 3.2 s (306 ms, 407 ms, 420 ms )\n",
    "\n",
    "reader_partitioned.count() # number of files read: 16 | size of files read: 16.4 MiB | 305 ms (32 ms, 39 ms, 54 ms )\n",
    "\n",
    "reader_partitioned_repart.count() # number of files read: 1 | size of files read: 16.4 MiB | 179 ms (9 ms, 43 ms, 44 ms )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa725fe2",
   "metadata": {},
   "source": [
    "### JOIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca0cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (14000, \"Северный\"),\n",
    "    (11000, \"Южный\"),\n",
    "    (10000, \"Восточный\"),\n",
    "    (26000, \"Западный\"),\n",
    "    (56000, \"Центральный\")\n",
    "]\n",
    "\n",
    "region_df = spark.createDataFrame(data, schema='region_id long, name string')\n",
    "\n",
    "region_df.show()\n",
    "\n",
    "\n",
    "customs_data = spark\\\n",
    "                .read\\\n",
    "                .csv('data/customs_data.csv', header=True, sep=';')\n",
    "\n",
    "customs_data.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04ea5e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Отключим автоматический Broadcast JOIN\n",
    "import time\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b43cb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Замерим выполнение запроса без broadcast join\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "customs_data.join(region_df, customs_data.region==region_df.region_id, \"left\").count()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Elapsed time for join operation: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "18c58d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Замерим выполнение запроса c broadcast join\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "customs_data.join(F.broadcast(region_df), customs_data.region == region_df.region_id, \"left\").count()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Elapsed time for broadcast join operation: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d4acb",
   "metadata": {},
   "source": [
    "### Cache | Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "024cb06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "customs_data.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b53e3a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "customs_data.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e7fd8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "customs_data.persist(StorageLevel.DISK_ONLY).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6533fa05",
   "metadata": {},
   "source": [
    "### Repartition & Coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51d0ef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(1,'one'), (2,'two'), (3,'three'), (4,'four'),\n",
    "        (5,'five'), (6,'six'), (7, 'seven'), (8, 'eight'),\n",
    "        (9, 'nine')]\n",
    "\n",
    "df = spark.createDataFrame(data, ['id', 'number'])\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "acfb35ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Намеренно перемешаем и поделим на 8 разделов\n",
    "mix = df.repartition(8)\n",
    "mix.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "181e27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.repartition(3).rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "64355478",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.coalesce(3).rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9b58b742",
   "metadata": {},
   "outputs": [],
   "source": [
    "mix.toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb3702c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUT OF MEMORY\n",
    "\n",
    "d = spark.read.csv('data/customs_data.csv', header=True, sep='\\t')\n",
    "d.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1eceeb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
